{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rsTKXMhDWq9",
        "colab_type": "text"
      },
      "source": [
        "# **Project in Computer Science:**\n",
        "# **Identifying and Categorizing Offensive Language in Social Media (OffensEval)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afAS4kflms7h",
        "colab_type": "text"
      },
      "source": [
        "## **First steps**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGBp09pLijLF",
        "colab_type": "text"
      },
      "source": [
        "The next cell, imports the *keras* module that will be essential to run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "423O6oN8LRFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLBnsLV5Db0h",
        "colab_type": "text"
      },
      "source": [
        "For the project, the code was run using Google CoLab. It is recommended to run the code using CoLab, due to the high computational cost that the program requires. Then, it is necessary to import the directory where the dataset is stored on the drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lduk-ElcLlq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xec5J83_DiIn",
        "colab_type": "text"
      },
      "source": [
        "The next cell imports all the necessary modules to run the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRZsSTLSLnXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORT MODULES\n",
        "import numpy as np \n",
        "import re\n",
        "import pandas as pd \n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, SimpleRNN, Embedding, concatenate, Input, LSTM, SpatialDropout1D, Dropout, GRU, Bidirectional, Flatten, Conv1D, GlobalMaxPooling1D, Convolution1D, MaxPooling1D, AveragePooling1D, Concatenate, Activation \n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setRku4EDot3",
        "colab_type": "text"
      },
      "source": [
        "In the cell bellow, all the necessary constants of the program are initialized. NOTE that the files path should be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR1aWsZ2LpqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# INITIALIZATION\n",
        "train_file = 'drive/My Drive/Dataset/olid-training-v1.0.tsv' # set corresponding path for the train dataset file\n",
        "test_file = 'drive/My Drive/Dataset/testset-levela.tsv' # set corresponding path for the test dataset file\n",
        "test_labels_a = 'drive/My Drive/Dataset/labels-levela.csv' # set corresponding path for the labels of the test dataset file\n",
        "\n",
        "models = list() # used to save all the models\n",
        "ensemble_model = list() # used to ensemble models\n",
        "\n",
        "maxlen = 200\n",
        "max_fatures = 10000\n",
        "\n",
        "# GloVe Pretrained Embeddings\n",
        "embedding_dim = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh_ayRuRmy8D",
        "colab_type": "text"
      },
      "source": [
        "## **Data preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EFPPjCGDwyG",
        "colab_type": "text"
      },
      "source": [
        "The data preprocessing is applied at the cell bellow. The techniques applied are:\n",
        "- Remove all the special characters and instances of USER and URL.\n",
        "- Split the hashtag into words.\n",
        "- Lowercase all the tweets.\n",
        "- Tokenization.\n",
        "\n",
        "NOTE that in this cell, the train dataset is split into a train and validation dataset. The validation dataset is the 10% of the original train dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v0Kyo4tMHt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPROCESSING\n",
        "\n",
        "# SPLIT HASHTAG INTO WORDS UTIL FUNCTIONS\n",
        "def replace_hashtag(tweet):\n",
        "  hashtags = find_hashtag(tweet)\n",
        "  for hashtag in hashtags:\n",
        "    split = split_hashtag(hashtag)\n",
        "    tweet = tweet.replace(hashtag, split)\n",
        "  tweet = tweet.replace('#', '')\n",
        "  return tweet\n",
        "\n",
        "def find_hashtag(tweet):\n",
        "  hashtags = re.findall(r\"#(\\w+)\", tweet)\n",
        "  return hashtags\n",
        "\n",
        "def split_hashtag(hashtag):\n",
        "  fo = re.compile(r'#[A-Z]{2,}(?![a-z])|[A-Z][a-z]+')\n",
        "  fi = fo.findall(hashtag)\n",
        "  result = ''\n",
        "  for var in fi:\n",
        "    result += var + ' '\n",
        "  return result\n",
        "\n",
        "# IMPORT DATASET\n",
        "data = pd.read_csv(train_file, sep='\\t', header=0)\n",
        "data = data[['id','tweet', 'subtask_a']]\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "# split hashtags into words\n",
        "for i, j in data.iterrows():\n",
        "  data.at[i,'tweet'] = replace_hashtag(j['tweet'])\n",
        "\n",
        "data['tweet'] = data['tweet'].apply(lambda x: x.lower()) # lowercase\n",
        "data['tweet'] = data['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove special characters\n",
        "data['tweet'] = data['tweet'].str.replace('user','') # remove 'user' tokens\n",
        "data['tweet'] = data['tweet'].str.replace('url','') # remove 'url' tokens\n",
        "\n",
        "# REAL TEST DATASET\n",
        "data_test = pd.read_csv(test_file, sep='\\t', header=0)\n",
        "\n",
        "# DATA PREPROCESSING\n",
        "# split hashtags into words\n",
        "for i, j in data_test.iterrows():\n",
        "  data_test.at[i,'tweet'] = replace_hashtag(j['tweet'])\n",
        "\n",
        "data_test['tweet'] = data_test['tweet'].apply(lambda x: x.lower()) # lowercase\n",
        "data_test['tweet'] = data_test['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x))) # remove special characters\n",
        "data_test['tweet'] = data_test['tweet'].str.replace('user','') # remove 'user' tokens\n",
        "data_test['tweet'] = data_test['tweet'].str.replace('url','') # remove 'url' tokens\n",
        "\n",
        "labels_test = pd.read_csv(test_labels_a, sep=',', header=0)\n",
        "labels_test = labels_test[['id','subtask_a']]\n",
        "data_test = pd.merge(data_test, labels_test, on='id')\n",
        "\n",
        "# TOKENIZER\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['tweet'].values)\n",
        "X = tokenizer.texts_to_sequences(data['tweet'].values)\n",
        "X = pad_sequences(X, maxlen=maxlen)\n",
        "Y = pd.get_dummies(data['subtask_a']).values\n",
        "\n",
        "# Testing with validation dataset\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
        "\n",
        "# Testing with original test dataset\n",
        "X_test_real = tokenizer.texts_to_sequences(data_test['tweet'].values)\n",
        "X_test_real = pad_sequences(X_test_real, maxlen=maxlen)\n",
        "Y_test_real = pd.get_dummies(data_test['subtask_a']).values\n",
        "\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)\n",
        "print(X_test_real.shape,Y_test_real.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RarTXvRBm4ay",
        "colab_type": "text"
      },
      "source": [
        "## **Pre-trained word embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNRmzjm4D11F",
        "colab_type": "text"
      },
      "source": [
        "In the next cell, the pre-trained word embeddings are loaded, and the embedding matrix is obtained. NOTE that the file path for the embeddings has to be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "270N7dGpM0mG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PRETRAINED EMBEDDINGS\n",
        "\n",
        "#embedding_file = open('drive/My Drive/embeddings/glove.6B.100d.txt') # 100-dimensional pre-trained word embeddings - set the file path\n",
        "embedding_file = open('drive/My Drive/embeddings/glove.6B.200d.txt') # 200-dimensional pre-trained word embeddings - set the file path\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "for line in embedding_file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "embedding_file.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index)) \n",
        "\n",
        "embedding_matrix = np.zeros((max_fatures, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if i < max_fatures:\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iYLC2UUmXp9",
        "colab_type": "text"
      },
      "source": [
        "## **Training Deep Learning models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hsg9TX9EAtA",
        "colab_type": "text"
      },
      "source": [
        "The following cells are divided into the different deep learning models (each cell initializes a different model). \n",
        "\n",
        "NOTE that you can run only one of these cells each time to evaluate a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6cP65o8D7kP",
        "colab_type": "text"
      },
      "source": [
        "### SimpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SyMXxmVNDzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple RNN\n",
        "model_name = 'Simple RNN'\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_fatures, embedding_dim,input_length = maxlen)) # without pretrained embeddings\n",
        "model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) # with pretrained embeddings\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4zVY810ED5g",
        "colab_type": "text"
      },
      "source": [
        "### Simple LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kPrnObxNGM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple LSTM\n",
        "model_name = 'Simple LSTM'\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_features, 32)) # without pretrained embeddings\n",
        "model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) # with pretrained embeddings\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBrESBAnEF2O",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qFnYzspNvs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LSTM\n",
        "model_name = 'LSTM'\n",
        "\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_fatures, embedding_dim,input_length = maxlen)) # without pretrained embeddings\n",
        "model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) # with pretrained embeddings\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ea0VT1EHoj",
        "colab_type": "text"
      },
      "source": [
        "### BiLSTM + DROPOUT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI9kuygiNxDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BILSTM + DROPOUT\n",
        "model_name = 'BILSTM + DROPOUT'\n",
        "\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_fatures, embedding_dim, input_length = maxlen, trainable = True)) #without pretrained embeddings\n",
        "model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) #with pretrained embeddings\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Bidirectional(LSTM(lstm_out, return_sequences=True, recurrent_dropout=0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Bidirectional(LSTM(lstm_out, return_sequences=True, recurrent_dropout=0.25)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(2,activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O-KjhoSEKgr",
        "colab_type": "text"
      },
      "source": [
        "### CNN + GLOBAL MAX POOLING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE7HzHfUNzkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN + GLOBAL MAX POOLING\n",
        "model_name = 'CNN + GLOBAL MAX POOLING'\n",
        "\n",
        "model = Sequential()\n",
        "#model.add(Embedding(max_fatures, embedding_dim, input_length = maxlen, trainable = False)) #without pretrained embeddings\n",
        "model.add(Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)) #with pretrained embeddings\n",
        "model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwSxtCiAEOwH",
        "colab_type": "text"
      },
      "source": [
        "### CNN (3 filters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MImoID1_N3A5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN (3 filters)\n",
        "model_name = 'CNN'\n",
        "\n",
        "embedding_dim = 200\n",
        "\n",
        "i = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
        "x = Embedding(max_fatures, embedding_dim ,weights = [embedding_matrix], input_length=maxlen, trainable=True)(i)\n",
        "x = Dropout(0.4)(x)\n",
        "\n",
        "def get_conv_pool(x_input, max_len, sufix, n_grams=[2,3,4], feature_maps=256):\n",
        "      branches = []\n",
        "      for n in n_grams:\n",
        "          branch = Conv1D(filters=feature_maps, kernel_size=n, activation='relu', name='Conv_'+sufix+'_'+str(n))(x_input)\n",
        "          branch = MaxPooling1D(pool_size=max_len-n+1, strides=None, padding='valid', name='MaxPooling_'+sufix+'_'+str(n))(branch)\n",
        "          branch = Flatten(name='Flatten_'+sufix+'_'+str(n))(branch)\n",
        "          branches.append(branch)\n",
        "      return branches\n",
        "\n",
        "branches = get_conv_pool(x, maxlen, 'dynamic')\n",
        "z = concatenate(branches, axis=-1)\n",
        "z1 = Dropout(0.3)(z)\n",
        "z2 = Dense(256, activation='relu')(z1)\n",
        "o = Dense(2, activation='softmax')(z2)\n",
        "\n",
        "model = Model(inputs=i, outputs=o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpBkl6FBESmZ",
        "colab_type": "text"
      },
      "source": [
        "### BiLSTM + BiGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRTVzEqKN6ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BILSTM + BIGRU\n",
        "model_name = 'BILSTM + BIGRU'\n",
        "\n",
        "lstm_units = 196\n",
        "gru_units = 64\n",
        "\n",
        "i = Input(shape=(maxlen,), dtype='int32', name='main_input')\n",
        "x = Embedding(max_fatures, embedding_dim, weights = [embedding_matrix], input_length=maxlen, trainable=True)(i)\n",
        "x = Dropout(0.4)(x)\n",
        "x1 = Bidirectional(LSTM(lstm_units, return_sequences=True, recurrent_dropout=0.3))(x)\n",
        "x2 = Dropout(0.3)(x1)\n",
        "x3 = Bidirectional(GRU(gru_units, return_sequences=True))(x2)\n",
        "x4 = Dropout(0.3)(x3)\n",
        "\n",
        "max_pooling = MaxPooling1D()(x4)\n",
        "max_pooling = Flatten()(max_pooling)\n",
        "\n",
        "average_pooling = AveragePooling1D()(x4)\n",
        "average_pooling = Flatten()(average_pooling)\n",
        "\n",
        "z1 = concatenate([max_pooling, average_pooling], axis=-1)\n",
        "z2 = Dense(128, activation='relu')(z1)\n",
        "o = Dense(2, activation='softmax')(z2)\n",
        "\n",
        "model = Model(inputs=i, outputs=o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xABhfipEVzo",
        "colab_type": "text"
      },
      "source": [
        "## **Evaluating deep learning models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erDrdIHGsCQq",
        "colab_type": "text"
      },
      "source": [
        "The following code has to be run each time you want to fit and evaluate a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT3nhlzFn8Sk",
        "colab_type": "text"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aSP-2LoTHZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COMPILE MODEL\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX2mdcS1EdJZ",
        "colab_type": "text"
      },
      "source": [
        "### Ploting the model scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCrERr53TIQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MODEL SCHEME\n",
        "print('Model:', model_name)\n",
        "\n",
        "print(model.summary())\n",
        "plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctgGQgnWEjoE",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4z1aRTjTLTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FIT THE MODEL\n",
        "print('Model:', model_name)\n",
        "\n",
        "batch_size = 128\n",
        "model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyLbfpFtEpkl",
        "colab_type": "text"
      },
      "source": [
        "The cell bellow, saves the models and creates a list for ensembling different models. In our project, the ensemble combines the BiLSTM+BiGRU model and the CNN (3 filters) model. \n",
        "\n",
        "To try the embedded model you have to run first one of the two models and save them in the *ensemble_model* list and then run the second model and save it again in the list. Finally, the *ensemble_model* list will contain the two models you want to ensemble."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIm6bAJ_idVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SAVE THE MODEL TO A LIST\n",
        "models.append(model) # save all the trained models\n",
        "\n",
        "# Ensemble of CNN (3 filters) and BiLSTM+BiGRU\n",
        "if model_name == 'BILSTM + BIGRU' or model_name == 'CNN':\n",
        "  ensemble_model.append(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfG7TMb9Et9M",
        "colab_type": "text"
      },
      "source": [
        "### Prediction for only one model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRPRlo6eo2wB",
        "colab_type": "text"
      },
      "source": [
        "Predict and plot the results of the prediction using the validation dataset and the test dataset. \n",
        "\n",
        "NOTE that you don't have to run this cell if you want to try an ensemble model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXa6WsQuTNe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION FOR ONE MODEL\n",
        "\n",
        "# PREDICTION USING VALIDATION DATASET\n",
        "print('Model:', model_name)\n",
        "print('Model prediction with validation dataset:')\n",
        "\n",
        "#Y_pred = model.predict_classes(X_test,batch_size = batch_size)\n",
        "\n",
        "# Prediction for class Model\n",
        "#Y_pred = model.predict(X_test,batch_size = batch_size)\n",
        "Y_pred = np.argmax(Y_pred,axis=1)\n",
        "\n",
        "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': Y_pred})\n",
        "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "print(\"Confusion matrix:\", confusion_matrix(df_test.true, df_test.pred))\n",
        "print(classification_report(df_test.true, df_test.pred, digits=4))\n",
        "\n",
        "# PREDICTION USING REAL TEST DATASET\n",
        "print('Model:', model_name)\n",
        "print('Model prediction with test dataset:')\n",
        "\n",
        "#Y_pred = model.predict_classes(X_test_real,batch_size = batch_size)\n",
        "\n",
        "# Prediction for class Model\n",
        "#Y_pred_real = model.predict(X_test_real,batch_size = batch_size)\n",
        "Y_pred_real = np.argmax(Y_pred_real,axis=1)\n",
        "\n",
        "df_test_real = pd.DataFrame({'true': Y_test_real.tolist(), 'pred':Y_pred_real})\n",
        "df_test_real['true'] = df_test_real['true'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "print(\"Confusion matrix\", confusion_matrix(df_test_real.true, df_test_real.pred))\n",
        "print(classification_report(df_test_real.true, df_test_real.pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9VCGecZFEDL",
        "colab_type": "text"
      },
      "source": [
        "### Prediction for ensembling models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwnlzkzBpE8j",
        "colab_type": "text"
      },
      "source": [
        "Predict and plot the results of the prediction using the validation dataset and the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjddbTt7ZvKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PREDICTION FOR ENSEMBLE MODELS\n",
        "\n",
        "# PREDICTION USING VALIDATION DATASET\n",
        "y_combine = [model.predict(X_test, batch_size = batch_size) for model in ensemble_model]\n",
        "y_combine = np.array(y_combine)\n",
        "\n",
        "# sum across ensembles\n",
        "summed = np.sum(y_combine, axis=0)\n",
        "\n",
        "# argmax across classes\n",
        "Y_pred = np.argmax(summed, axis=1)\n",
        "\n",
        "df_test = pd.DataFrame({'true': Y_test.tolist(), 'pred': Y_pred})\n",
        "df_test['true'] = df_test['true'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "print(\"Confusion matrix:\", confusion_matrix(df_test.true, df_test.pred))\n",
        "print(classification_report(df_test.true, df_test.pred, digits=4))\n",
        "\n",
        "# PREDICTION USING REAL TEST DATASET\n",
        "y_combine = [model.predict(X_test_real, batch_size = batch_size) for model in ensemble_model]\n",
        "y_combine = np.array(y_combine)\n",
        "\n",
        "# sum across ensembles\n",
        "summed = np.sum(y_combine, axis=0)\n",
        "\n",
        "# argmax across classes\n",
        "Y_pred_real = np.argmax(summed, axis=1)\n",
        "\n",
        "df_test_real = pd.DataFrame({'true': Y_test_real.tolist(), 'pred':Y_pred_real})\n",
        "df_test_real['true'] = df_test_real['true'].apply(lambda x: np.argmax(x))\n",
        "\n",
        "print(\"Confusion matrix\", confusion_matrix(df_test_real.true, df_test_real.pred))\n",
        "print(classification_report(df_test_real.true, df_test_real.pred, digits=4))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}